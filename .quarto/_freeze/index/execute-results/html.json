{
  "hash": "0f7d45a86dae582a040feea9292fce9d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Time series models\" \nauthor: \"Barry Quinn <br> <span class='glowinline' Digital Dave</span>\"\nfooter: \"Advanced Financial Data Analytics\"\ncss: mycssblend.css\nlogo: img/qbslogo.png\nexecute: \n  echo: true\nformat: \n  revealjs:\n    slide-number: c/t\n    scrollable: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n\n## Learning outcomes\n\n::: saltinline\n-   Stationary and differencing\n-   Modelling stationary time series\n-   ARIMA models (liner model)\n-   Categories of algorithms in capital markets\n-   Algorithmic ARIMA modelling\n-   Practical example with some of the project data\n:::\n\n## Stationarity and differencing\n\n::: acidinline\n-   The foundation of statistical inference in time series analysis is\n    the concept of weak stationarity.\n-   roughly horizontal\n-   constant variance\n-   no patterns predictable in the long-term\n:::\n\n##  {.your-turn}\n\nAre these financial time series stationary?\n\n::: columns\n::: column\n\n::: {.cell}\n\n```{.r .cell-code}\ntsfe::indices %>%\n  select(date,`RUSSELL 2000 - PRICE INDEX`) %>%\n  rename(r2000=`RUSSELL 2000 - PRICE INDEX`) %>%\n  drop_na() %>%\n  tq_transmute(select =r2000,mutate_fun = periodReturn,type='log') ->monthly_r2002r\n  ts(monthly_r2002r$monthly.returns, start = c(1988,1))->r2000r_m_ts\n  autoplot(r2000r_m_ts) + ylab(\"Log returns\") + xlab(\"Year\") + labs(title=\"Figure 2: Monthly log returns of the Russell 2000 Price Index\",subtitle =\" from March 1988 to December 2019\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n:::\n\n::: column\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tsfe::carnival_eps_ts) + xlab(\"Year\") + ylab(\"Earnings\") +\n  labs(title=\"Figure 3:\",subtitle =  \"Quarterly earnings per share for Carnival Plc from the first quarter of 1994 to the fourth quarter of 2019\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Inference and stationarity {.smaller}\n\n-   The monthly log returns of Russell 2000 index vary around zero over\n    time.\n    -   If we divide up the data into subperiods we would expect each\n        sample mean to be roughly zero.\n-   Furthermore, expect the recent financial crisis (2007-2009), the log\n    returns range is approximately \\[-0.2,0.2\\].\n-   Statistically, the mean and the variance are constant over time OR\n    time invariant.\n-   Put together these to time invariant properties characterise a\n    weakly stationary series.\n\n## Weak stationarity and prediction\n\n-   Weak form stationarity provides a basic framework for prediction.\n-   For the monthly log returns of the Russell 2000 we can predict with\n    reasonable confidence:\n-   Future monthly returns $\\approx0$ and vary $[-0.2,0.2]$\n\n## Inference and nonstationarity\n\n-   Consider quarterly earnings for Carnival Plc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fpp2)\ntsfe::carnival_eps_ts |> autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## Inference and nonstationarity\n\n-   If the timespan is divided into subperiods the sample mean and\n    variance for each period show increasing pattern.\n-   Earnings are **not** weakly stationary.\n-   There does exist models and methods for modelling such nonstationary\n    series.\n\n::: your-turn\n## Is the VIX time series Stationary?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntq_get('^VIX') %>%\n  ggplot(aes(x=date,y=adjusted)) + \n  ylab(\"VIX\") + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n:::\n\n## Non-stationarity in the mean\n\n-   Identifying non-stationary series:\n\n-   time plot.\n\n-   The ACF of stationary data drops to zero relatively quickly\n\n-   The ACF of non-stationary data decreases slowly.\n\n-   For non-stationary data, the value of $r_1$ is often large and\n    positive.\n\n## Example: FTSE index\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(ftse_m_ts) + ylab(\"Monthly Price Index\") + xlab(\"Year\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## ACF\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggAcf(ftse_m_ts)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n## First differencing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(diff(ftse_m_ts)) + ylab(\"Change in monthly FTSE Index\") + xlab(\"Year\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n### ACF after first differencing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggAcf(diff(ftse_m_ts))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## Differencing {.saltinline}\n\n-   Differencing helps to **stabilize the mean**.\n-   The differenced series is the *change* between each observation in\n    the original series: ${y'_t = y_t - y_{t-1}}$.\n-   The differenced series will have only $T-1$ values since it is not\n    possible to calculate a difference $y_1'$ for the first observation.\n\n## Financial ratio example\n\n### subset using `window()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwindow(carnival_eps_ts,end=c(2010,1)) %>% autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/carnival1-1.png){width=960}\n:::\n:::\n\n\n## use `log()` to stablise variation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwindow(carnival_eps_ts,end=c(2010,1)) %>% log() %>% autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/carnival2-1.png){width=960}\n:::\n:::\n\n\n## Seasonally differencing?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwindow(carnival_eps_ts,end=c(2010,1)) %>% \n  log() %>% diff(lag=4) %>%  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/carnival3-1.png){width=960}\n:::\n:::\n\n\n## Is this now stationary {.your-turn}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwindow(carnival_eps_ts,end=c(2010,1)) %>% log() %>% diff(lag=4) %>% diff(lag=1) %>% autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/carnival4-1.png){width=960}\n:::\n:::\n\n\n##  {.discussion .small}\n\n-   Seasonally differenced series is closer to being stationary.\n-   Remaining non-stationarity can be removed with further first\n    difference.\n-   If $y'_t = y_t - y_{t-12}$ denotes seasonally differenced series,\n    then twice-differenced series i\n-   When both seasonal and first differences are applied\n-   it makes no difference which is done first the result will be the\n    same.\n-   If seasonality is strong, we recommend that seasonal differencing be\n    done first because sometimes the resulting series will be stationary\n    and there will be no need for further first difference.\n-   It is important that if differencing is used, the differences are\n    interpretable.\n\n## Interpretation of differencing {.heatinline}\n\n-   first differences are the change between **one observation and the\n    next**;\n\n-   seasonal differences are the change between **one year to the\n    next**.\n\n-   But taking lag 3 differences for yearly data, for example, results\n    in a model which cannot be sensibly interpreted.\n\n## Unit root tests\n\n> Statistical tests to determine the required order of differencing\n\n1.  Augmented Dickey Fuller test: null hypothesis is that the data are\n    non-stationary and non-seasonal.\n2.  Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test: null hypothesis is\n    that the data are stationary and non-seasonal.\n3.  Other tests available for seasonal data.\n\n## KPSS test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(urca)\nsummary(ur.kpss(ftse_m_ts))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 3 lags. \n\nValue of test-statistic is: 0.3983 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n```\n\n\n:::\n:::\n\n\n## Automatically selecting differences\n\n-   Seasonal strength\n    $F_s = \\max\\big(0, 1-\\frac{\\text{Var}(R_t)}{\\text{Var}(S_t+R_t)}\\big)$\n\n-   If $F_s > 0.64$, do one seasonal difference.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncarnival_eps_ts %>% log() %>% nsdiffs()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncarnival_eps_ts %>% log() %>% diff(lag=4) %>% ndiffs()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n## Non-seasonal ARIMA models {.smaller}\n\n#### Autoregressive models\n\n-   When $y_t$ has a statistically significant lag-1 autocorrelation,\n    the lagged value $y_{t-1}$ might be a useful in predicting $y_t$.\n-   AR(1) model\n\n$$y_{t}= c+\\phi_{1}y_{t - 1} + \\varepsilon_{t}$$\n\n-   where $\\varepsilon_t$ is white noise.\\\n\n-   This is a simple linear regression with **lagged values** of $y_t$\n    as predictors.\n\n-   This simple model is widely used in stochastic volatility when $y_t$\n    is replaced by its log volatility.\n\n## Autoregressive models\n\n-   More generally, if the $E(y_{t-1})$ is determined by more than lag-1\n    we can generalise a AR(1) to an AR(p) model.\n\n-   $$y_{t}= c+\\phi_{1}y_{t - 1}+\\phi_{2}y_{t - 2} + \\dots +\\phi_{p}y_{t - p}  + \\varepsilon_{t},$$\n    where $\\varepsilon_t$ is white noise.\\\n\n-   This is a multiple linear regression with **lagged values** of $y_t$\n    as predictors.\n\n## Example of an AR(1) model\n\n::: columns\n::: column\n-   Simulating an $y_{t} =2 -0.8 y_{t - 1}+\\varepsilon_{t}$\n-   where $\\varepsilon_t\\sim N(0,1)$ for $T=100$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nautoplot(10 + arima.sim(list(ar = -0.8), n = 100)) +\n  ylab(\"\") + ggtitle(\"AR(1)\")\n```\n:::\n\n:::\n\n::: column\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/arp1-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Simulating an AR(2)\n\n::: columns\n::: {.column .small}\n-   Simulating an $y_t = 8 + 1.3y_{t-1} - 0.7 y_{t-2} + \\varepsilon_t$\n\n-   where $\\varepsilon_t\\sim N(0,1)$ for $T=100$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nautoplot(20 + arima.sim(list(ar = c(1.3, -0.7)), n = 100)) + ylab(\"\") + ggtitle(\"AR(2)\")\n```\n:::\n\n:::\n\n::: column\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/arp4-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## AR(1) models explained {.small}\n\n$$y_{t}=c + \\phi_1 y_{t -1}+\\varepsilon_{t}$$ When $\\phi_1=0$, $y_t$ is\n**equivalent to White Noise**\n\nWhen $\\phi_1=1$ and $c=0$, $y_t$ is **equivalent to a Random Walk**\n\nWhen $\\phi_1=1$ and $c\\ne0$, $y_t$ is **equivalent to a Random Walk with\ndrift**\n\nWhen $\\phi_1<0$, $y_t$ tends to **oscillate between positive and\nnegative values**.\n\n## Moving Average (MA) models\n\n-   Moving Average (MA) models:\n\n$$y_{t} =c +\\varepsilon_t + \\theta_{1}\\varepsilon_{t - 1} + \\theta_{2}\\varepsilon_{t - 2} +\\cdots+\\theta_{q}\\varepsilon_{t - q},$$\n\nwhere $\\varepsilon_t$ is white noise. - This is a multiple regression\nwith **past errors** as predictors. **Don't confuse this with moving\naverage smoothing!**\n\n## Simulated MA model {.small}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\np1<-autoplot(20 + arima.sim(list(ma = 0.8), n = 100)) +\n  ylab(\"\") + ggtitle(TeX(r'(MA(2) model: $y_t = 20 + \\epsilon_t + 0.8 \\epsilon_{t-1} ... \\epsilon_t \\sim N(0,1)$)'))\np2 <- autoplot(arima.sim(list(ma = c(-1, +0.8)), n = 100)) +\n  ylab(\"\") + ggtitle(TeX(r\"(MA(2) model:  $y_t = \\epsilon_t -\\epsilon_{t-1} + 0.8 \\epsilon_{t-2} ... \\epsilon_t \\sim N(0,1)$)\"))\ngridExtra::grid.arrange(p1,p2,nrow=1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/maq-1.png){width=960}\n:::\n:::\n\n\n## ARIMA models\n\nAutoregressive Integrated Moving Average models\n\n::: fatinline\nAutoregressive ~~Integrated~~ Moving Average models (ARMA) $$\n\\begin{align*}\ny_{t} = c+ \\phi_{1}y_{t - 1} +\\cdots  +\\phi_{p}y_{t-p} \\\\\n \\theta_{1}\\varepsilon_{t - 1} + \\cdots +\\theta_{q}\\varepsilon_{t-q} +\\varepsilon_{t}.\n\\end{align*}\n$$\n:::\n\n## ARIMA models {.smaller}\n\n-   Predictors include both **lagged values of** $y_t$ and lagged\n    errors.\n-   Conditions on coefficients ensure stationarity.\n-   Conditions on coefficients ensure invertibility.\n-   Combine ARMA model with **differencing**.\n\n## ARIMA models notation\n\n-   ARIMA(p, d, q) model\n-   AR part: p = order of the autoregressive part\n-   I part: d = degree of first differencing involved\n-   MA part: q = order of the moving average part.\n-   ARIMA(1,1,1) model:\n    $$(y_t-y_{t-1}) =c + \\phi_1 (y_{t-1}- y_{t-2}) +\\theta_1\\varepsilon_{t-1} + \\varepsilon_t$$\n-   These componets are the integrated parts\n    $(y_t-y_{t-1}) \\dots (y_{t-1}- y_{t-2})$\n\n## ARIMA models notation\n\n-   White noise model: ARIMA(0,0,0)\n-   Random walk: ARIMA(0,1,0) with no constant\n-   Random walk with drift: ARIMA(0,1,0) with `constant term`\n-   AR(p): ARIMA(p,0,0)\n-   MA(q): ARIMA(0,0,q)\n\n## ARIMA modelling of US consumption\n\n### The data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(uschange[,\"Consumption\"]) +\n  xlab(\"Year\") +\n  ylab(\"Quarterly percentage change\") +\n  ggtitle(\"US consumption\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n## fit an ARIMA(2,0,2)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n((fit <- arima(uschange[,\"Consumption\"],order = c(2,0,2))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = uschange[, \"Consumption\"], order = c(2, 0, 2))\n\nCoefficients:\n         ar1      ar2      ma1     ma2  intercept\n      1.3908  -0.5813  -1.1800  0.5584     0.7463\ns.e.  0.2553   0.2078   0.2381  0.1403     0.0845\n\nsigma^2 estimated as 0.3417:  log likelihood = -165.14,  aic = 342.28\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef <- coefficients(fit)\nar1=round(coef['ar1'],3) %>% unname()\nar2=round(coef['ar2'],3) %>% unname()\nintercept = round(coef['intercept'] * (1-coef['ar1'] - coef['ar2']),3) %>% unname()\nma1=round(coef['ma1'],3) %>% unname()\nma2=round(coef['ma2'],3) %>% unname()\nsigma=round(sqrt(fit$sigma2),3) %>% unname()\nsigma2=round(fit$sigma2,3) %>% unname()\n```\n:::\n\n\n## The estimated model\n\n-   $y_t = c + 1.391y_{t-1} -0.581y_{t-2}-1.18 \\varepsilon_{t-1}+ 0.558\\varepsilon_{t-2}+ \\varepsilon_{t}$\n-   where $c= 0.142$\n-   and $\\varepsilon_t$ is white noise with a standard deviation of\n    $0.585 = \\sqrt{0.342}$.\n\n## Forecasts from estimated model\n\n### The forecasted data\n\n-   The data is point estimates based on some frequentist asymptotic\n    theory\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit %>% forecast(h=10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Point Forecast        Lo 80    Hi 80      Lo 95    Hi 95\n2016 Q4      0.7502795  0.001108855 1.499450 -0.3954781 1.896037\n2017 Q1      0.7894265  0.023789630 1.555063 -0.3815141 1.960367\n2017 Q2      0.8039764  0.012001551 1.595951 -0.4072446 2.015197\n2017 Q3      0.8014580 -0.012967301 1.615883 -0.4440980 2.047014\n2017 Q4      0.7894979 -0.037980785 1.616976 -0.4760215 2.055017\n2018 Q1      0.7743270 -0.058387145 1.607041 -0.4991994 2.047853\n2018 Q2      0.7601787 -0.073727533 1.594085 -0.5151708 2.035528\n2018 Q3      0.7493189 -0.084623827 1.583262 -0.5260865 2.024724\n2018 Q4      0.7424386 -0.091640109 1.576517 -0.5331747 2.018052\n2019 Q1      0.7391815 -0.095286207 1.573649 -0.5370267 2.015390\n```\n\n\n:::\n:::\n\n\n## Forecasts from estimated model\n\n### Plotting the forecasts\n\n-   The points estimates are visualised as banded limits at the 80% and\n    95% forecasting levels\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit %>% forecast(h=10) %>% autoplot(include=80)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n## {.your-turn .small}\n### What is the Correct Interpretation of a 95% Confidence Interval for a Population Mean?\n\n- **A.** There is a 95% probability that the population mean falls within the calculated confidence interval.\n- **B.** If we were to draw 100 different samples and compute a 95% confidence interval for each sample, we would expect about 95 of these intervals to contain the population mean.\n- **C.** The population mean is 95% likely to be the center point of the calculated confidence interval.\n- **D.** There is a 95% chance that any given sample mean falls within the calculated confidence interval.\n- **E.** The calculated confidence interval captures the range within which 95% of the population data falls.\n\n## {.your-turn .small .hidden}\n### What is the Correct Interpretation of a 95% Confidence Interval for a Population Mean?\n\n- **A.** There is a 95% probability that the population mean falls within the calculated confidence interval.\n- **B.** If we were to draw 100 different samples and compute a 95% confidence interval for each sample, we would expect about 95 of these intervals to contain the population mean. **(Correct)**\n- **C.** The population mean is 95% likely to be the center point of the calculated confidence interval.\n- **D.** There is a 95% chance that any given sample mean falls within the calculated confidence interval.\n- **E.** The calculated confidence interval captures the range within which 95% of the population data falls.\n\n## Understanding ARIMA models {.small}\n\n-   If $c=0$ and $d=0$, the long-term forecasts will go to zero.\n-   If $c=0$ and $d=1$, the long-term forecasts will go to a non-zero\n    constant.\n-   If $c=0$ and $d=2$, the long-term forecasts will follow a straight\n    line.\n-   If $c\\ne0$ and $d=0$, the long-term forecasts will go to the mean of\n    the data.\n-   If $c\\ne0$ and $d=1$, the long-term forecasts will follow a straight\n    line.\n-   If $c\\ne0$ and $d=2$, the long-term forecasts will follow a\n    quadratic trend.\n\n## Understanding ARIMA models\n\n### Forecast variance and $d$\n\n-   The higher the value of $d$, the more rapidly the prediction\n    intervals increase in size.\n-   For $d=0$, the long-term forecast standard deviation will go to the\n    standard deviation of the historical data.\n\n## Understanding ARIMA models {.smaller}\n\n### Cyclic behaviour\n\n-   For cyclic forecasts, $p\\ge2$ and some restrictions on coefficients\n    are required.\n-   If $p=2$, we need $\\phi_1^2+4\\phi_2<0$. Then average length of\n    stochastic cycles is\n\n$$(2\\pi)/\\left[\\text{arc cos}(-\\phi_1(1-\\phi_2)/(4\\phi_2))\\right].$$\n\n-   This formula has important uses in estimation business and economic\n    cycles. (See Example 2.3 in Tsay (2010))\n\n## Model building {.small}\n\n### Maximum likelihood estimation  (MLE)\n\n-   Having identified the model order, we need to estimate the\n    parameters $c,\\phi_1,\\dots,\\phi_p \\text{  }\\theta_1,\\dots,\\theta_q$.\n\n-   MLE is very similar to least squares estimation obtained by\n    minimizing $\\sum_{t-1}^T e_t^2$\n\n-   The `Arima()` command allows MLE estimation and constrained least squares(CLS).\n\n-   Non-linear optimization must be used in either case.\n\n-   Different software will give different estimates.\n\n## Partial autocorrelations {.small}\n\n-   Partial autocorrelations} measure relationship between $y_{t}$ and\n    $y_{t - k}$, when the effects of other time lags\n    $1,2, 3, \\dots, k - 1$are removed.\n\n-   $\\alpha_k$= $k$th partial autocorrelation coefficient\n\n-   $\\alpha_k${equal to the estimate of $b_k$ in regression:\n\n$$y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_k y_{t-k}$$\n\n-   Varying number of terms on RHS gives $\\alpha_k$ for different values\n    of $k$.\n-   There are more efficient ways of calculating $\\alpha_k$.\n-   $\\alpha_1=\\rho_1$\n-   same critical values of $\\pm 1.96/\\sqrt{T}$ as for ACF.\n\n## Example: US consumption\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- ggAcf(uschange[,\"Consumption\"],main=\"\")\np2 <- ggPacf(uschange[,\"Consumption\"],main=\"\")\ngridExtra::grid.arrange(p1,p2,nrow=1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/usconsumptionacf-1.png){width=960}\n:::\n:::\n\n\n## ACF and PACF interpretation\n\n**AR(1)** $$rho_k =\\phi_1^k \\text{  for k=1,2,}\\dots$$\n\n$$\\alpha_1= \\phi_1 \\alpha_k = 0\\text{for k=2,3}\\dots$$\n\nSo we have an AR(1) model when\n\n-   autocorrelations exponentially decay\n-   there is a single significant partial autocorrelation.\n\n## ACF and PACF interpretation{.small}\n\n- **AR(p)** \n- ACF dies out in an exponential or damped sine-wave manner\n- PACF has all zero spikes beyond the $p$th spike\n\nSo we have an AR(p)) model when\n\n- the ACF is exponentially decaying or sinusoidal\n- there is a significant spike at lag $p$ in PACF, but none beyond $p$\n\n## **MA(1)** based on ACF and PACF \n\n$$\n\\begin{align*}\n\\rho_1 &= \\theta_1 \\qquad \\rho_k = 0\\qquad\\text{for k=2,3,...};\\\\\n\\alpha_k &= -(-\\theta_1)^k\n\\end{align*}\n$$ \n\n- So we have an MA(1) model when \n- the PACF is exponentially decaying\n\n- and there is a single significant spike in ACF\n\n##  More generally picking q for **MA(q)**\n\n-   PACF dies out in an exponential or damped sine-wave manner\n-   ACF has all zero spikes beyond the $q$th spike\n\nSo we have an MA(q) model when\n\n-   the PACF is exponentially decaying or sinusoidal\n-   there is a significant spike at lag $q$ in ACF, but none beyond $q$\n\n## Information criteria for model selection {.smaller}\n\n-   In advanced financial modelling we use information theory to\n    scientifically determine which of our model choices contains the\n    most statistical information.\n\n**Akaike's Information Criterion (AIC):**\n\n$\\text{AIC} = -2 \\log(L) + 2(p+q+k+1),$ where $L$ is the likelihood of\nthe data, $k=1$ if $c\\ne0$ and $k=0$ if $c=0$\\]\n\n**Corrected AIC:** <br>\n$\\text{AICc} = \\text{AIC} + \\frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2}.$\n\n## Information criteria for model selection {.smaller}\n\n**Bayesian Information Criterion:**<br>\n$\\text{BIC} = \\text{AIC} + [\\log(T)-2](p+q+k-1).$<br> Good models are\nobtained by minimizing either the AIC, AICc or BIC. My preference is to\nuse the AICc.\n\n## Powerful non-stationary model in finance\n\n-   In financial time series an important class on non-stationary times\n    series model is the random walk model\n-   A random walk can be define as $y_t=y_{t-1}+ error_t$ or its drift\n    variation $y_t= constant + y_{t-1}+ error_t$\n\n## Simulation 1\n\n$y_t = 10 + 0.99y_{t-1}+ \\varepsilon_t$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nautoplot(10 + arima.sim(list(ar =0.99), n = 100))  + ylab(\"\") + \n  ggtitle(\"Is this a random walk with drift?\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n## Simulation 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\nS0=10\nn=100\nchgs=rnorm(n-1,1.001,0.01)\nrw=ts(cumprod(c(S0,chgs)))\nautoplot(rw)  + ylab(\"\") + \n  ggtitle(\"Is this a random walk with drift?\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n## AI automation for ARIMA models\n\n-   `auto.arima()`\n\n-   For a non-seasonal ARIMA process we first need to select appropriate\n    orders: $p,q,d$\n\n-   We use the [Hyndman and Khandakar (JSS,\n    2008)](https://www.jstatsoft.org/article/view/v027i03) algorithm:\n\n-   Select no. differences $d$ and $D$ via KPSS test and seasonal\n    strength measure.\n\n-   Select $p,q$ by minimising AICc.\n\n-   Use stepwise search to traverse model space.\n\n## How does auto.arima() work? {.small}\n\n$$\\text{AICc} = -2 log(L) + 2(p+q+k+1)\\left[1 +\n\\frac{(p+q+k+2)}{T-p-q-k-2}\\right].$$\n\n-   where $L$ is the maximised likelihood fitted to the differenced\n    data,\n\n-   $k=1$ if $c\\neq 0$ and $k=0$ otherwise.\n\n## Algorithmic rule 1:\n\n-   Select current model (with smallest AICc) from:\n-   ARIMA$(2,d,2)$\n-   ARIMA$(0,d,0)$\n-   ARIMA$(1,d,0)$\n-   ARIMA$(0,d,1)$\n\n## Algorithmic rule 2:\n\n-   Consider variations of current model:\n\n-   vary one of $p,q,$ from current model by $\\pm1$;\n\n-   $p,q$ both vary from current model by $\\pm1$;\n\n-   Include/exclude $c$ from current model.\n\n::: heatinline\n-   Model with lowest AICc becomes current model.\n-   Repeat rule 2 until no lower AICc can be found\n:::\n\n## Example: VIX index\n\n## Get Data\n\n::: columns\n::: {.column}\n\n::: {.cell}\n\n```{.r .cell-code}\ntidyquant::tq_get('^VIX') %>%\n  select(adjusted) %>%\n  ts() -> vix_ts\nggtsdisplay(log(vix_ts))\n```\n:::\n\n:::\n\n::: {.column}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Human choice\n\n::: columns\n::: {.column}\n\n::: {.cell}\n\n```{.r .cell-code}\nggtsdisplay(diff(log(vix_ts)))\n```\n:::\n\n:::\n\n::: {.column}\n\n::: {.cell}\n\n```{.r .cell-code}\nggtsdisplay(diff(log(vix_ts)))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Human choice\n\n::: columns\n::: {.column}\n\n::: {.cell}\n\n```{.r .cell-code}\n(fit <- Arima(log(vix_ts),order=c(7,1,0)))\n```\n:::\n\n:::\n\n::: {.column}\n\n::: {.cell}\n\n```{.r .cell-code}\n(fit <- Arima(log(vix_ts),order=c(7,1,0)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: log(vix_ts) \nARIMA(7,1,0) \n\nCoefficients:\n          ar1      ar2      ar3      ar4      ar5      ar6      ar7\n      -0.0727  -0.0564  -0.0126  -0.0586  -0.0088  -0.0523  -0.0438\ns.e.   0.0199   0.0199   0.0200   0.0200   0.0201   0.0202   0.0204\n\nsigma^2 = 0.006001:  log likelihood = 2878.42\nAIC=-5740.84   AICc=-5740.79   BIC=-5693.8\n```\n\n\n:::\n:::\n\n:::\n:::\n\n## Algorithmic choice\n\n::: columns\n::: {.column}\n\n::: {.cell}\n\n```{.r .cell-code}\nauto.arima(log(vix_ts))\n```\n:::\n\n:::\n\n::: {.column}\n\n::: {.cell}\n\n```{.r .cell-code}\nauto.arima(log(vix_ts))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: log(vix_ts) \nARIMA(0,1,2) with drift \n\nCoefficients:\n          ma1      ma2   drift\n      -0.0708  -0.0533  0.0000\ns.e.   0.0199   0.0212  0.0013\n\nsigma^2 = 0.006034:  log likelihood = 2869.5\nAIC=-5731.01   AICc=-5730.99   BIC=-5707.49\n```\n\n\n:::\n:::\n\n:::\n:::\n\n## More careful algorthmic choice\n\n::: columns \n:::{.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(fit.auto<-auto.arima(log(vix_ts), \n                      stepwise=FALSE,\n                      approximation=FALSE))\n```\n:::\n\n\n::: \n:::{.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(fit.auto<-auto.arima(log(vix_ts), \n                      stepwise=FALSE,\n                      approximation=FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: log(vix_ts) \nARIMA(3,1,2) \n\nCoefficients:\n         ar1     ar2     ar3      ma1      ma2\n      0.1210  0.7182  0.0616  -0.2029  -0.7729\ns.e.  0.1217  0.1156  0.0206   0.1205   0.1180\n\nsigma^2 = 0.005951:  log likelihood = 2888.36\nAIC=-5764.71   AICc=-5764.68   BIC=-5729.43\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n## {.discussion}\n-   Setting both `stepwise` and `approximation` arguments to `FALSE`\n    will slow the automation down but provides a more exhaustive search\n    for the appropriate model.\n-   The `auto.arima` function then searches over all possible models\n    using MLE.\n-   See `help(auto.arima)` for more details.\n\n## Human Vs Algo: Residual Diagnostics\n\n### Human choice\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheckresiduals(fit, test=FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n## Human forecasting\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit %>% forecast(h=252) %>% autoplot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n## Algorithm\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheckresiduals(fit.auto,test = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-30-1.png){width=960}\n:::\n:::\n\n\n## Algo forecasting\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit.auto %>% forecast(h=252) %>% autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-31-1.png){width=960}\n:::\n:::\n\n\n## Modelling procedure with `Arima` {.small}\n\n### This is sometimes referred to as the Box-Jenkins approach\n\n1.  Plot the data. Identify any unusual observations.\n2.  If necessary, transform the data (using a Box-Cox transformation) to\n    stabilize the variance.\n3.  If the data are non-stationary: take first differences of the data\n    until the data are stationary.\n4.  Examine the ACF/PACF: Is an AR(p) or MA(q) model appropriate?\n5.  Try your chosen model(s), and use the AICc to search for a better\n    model.\n6.  Check the residuals from your chosen model by plotting the ACF of\n    the residuals, and doing a portmanteau test of the residuals. If\n    they do not look like white noise, try a modified model.\n7.  Once the residuals look like white noise, calculate forecasts.\n\n## Modelling procedure with `auto.arima`\n\n1.  Plot the data. Identify any unusual observations.\n\n2.  If necessary, transform the data (using logs) to stabilize the\n    variance.\n\n3.  Use `auto.arima` to select a model.\n\n4.  Check the residuals from your chosen model by plotting the ACF of\n    the residuals, and doing a portmanteau test of the residuals. If\n    they do not look like white noise, try a modified model.\n\n5.  Once the residuals look like white noise, calculate forecasts.\n\n## Project based example {.small}\n\n::: columns\n::: column\n\n::: {.cell}\n\n```{.r .cell-code}\ntsfe::indices %>%\n  select(date,`RUSSELL 2000 - PRICE INDEX`) %>%\n  rename(r2000=`RUSSELL 2000 - PRICE INDEX`) %>%\n  drop_na() %>%\n  tq_transmute(select =r2000,mutate_fun = periodReturn,type='log') ->monthly_r2002r\n  ts(monthly_r2002r$monthly.returns, start = c(1988,1))->r2000r_m_ts\nautoplot(r2000r_m_ts) + xlab(\"Year\") +\n  ylab(\"monthly log returns\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-32-1.png){width=960}\n:::\n:::\n\n:::\n\n::: column\n1.  Time plot shows sudden changes, particularly big movements in\n    2007/2008 due to financial crisis. Otherwise nothing unusual and no\n    need for data adjustments.\n2.  Little evidence of changing variance, so no log transformation\n    needed.\n3.  Data are clearly stationary, so no *differencing* required.\n:::\n:::\n\n## Project based example\n\n::: columns \n:::{.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggtsdisplay(r2000r_m_ts)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/r2000r2-1.png){width=960}\n:::\n:::\n\n:::\n::: column\n4.  PACF is suggestive of AR(5). So initial candidate model is\n    ARIMA(5,0,0). No other obvious candidates.\n5.  Fit ARIMA(5,0,0) model along with variations: ARIMA(4,0,0),\n    ARIMA(3,0,0), ARIMA(4,0,1), etc. ARIMA(3,0,1) has smallest *AICc*\n    value.\n:::\n:::\n\n## Project based example\n\n### Manual fit\n\n::: columns\n::: column\n\n::: {.cell}\n\n```{.r .cell-code}\n(fit <- Arima(r2000r_m_ts, order=c(3,0,1)))\n```\n:::\n\n:::\n\n::: column\n\n::: {.cell}\n\n```{.r .cell-code}\n(fit <- Arima(r2000r_m_ts, order=c(3,0,1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: r2000r_m_ts \nARIMA(3,0,1) with non-zero mean \n\nCoefficients:\n         ar1      ar2      ar3      ma1    mean\n      1.0466  -0.0945  -0.0029  -1.0000  0.0062\ns.e.  0.0509   0.0736   0.0511   0.0094  0.0004\n\nsigma^2 = 0.002795:  log likelihood = 586.84\nAIC=-1161.68   AICc=-1161.46   BIC=-1137.96\n```\n\n\n:::\n:::\n\n:::\n:::\n\n## Residual check\n\n6.  ACF plot of residuals from ARIMA(3,0,1) model look like white noise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheckresiduals(fit, plot=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(3,0,1) with non-zero mean\nQ* = 7.5056, df = 6, p-value = 0.2766\n\nModel df: 4.   Total lags used: 10\n```\n\n\n:::\n:::\n\n\n## Forecasting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit %>% forecast(h=24) %>% autoplot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n\n## Understanding the ARIMA output {.small}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(fit <- Arima(r2000r_m_ts, order=c(3,0,1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: r2000r_m_ts \nARIMA(3,0,1) with non-zero mean \n\nCoefficients:\n         ar1      ar2      ar3      ma1    mean\n      1.0466  -0.0945  -0.0029  -1.0000  0.0062\ns.e.  0.0509   0.0736   0.0511   0.0094  0.0004\n\nsigma^2 = 0.002795:  log likelihood = 586.84\nAIC=-1161.68   AICc=-1161.46   BIC=-1137.96\n```\n\n\n:::\n:::\n\n\n## Understanding the ARIMA output {.small}\n\n\n::: {.cell}\n\n```{.r .cell-code}\na=unname(round(fit$coef[\"intercept\"],3))\nar1=unname(round(fit$coef[\"ar1\"],3))\nar2=unname(round(fit$coef[\"ar2\"],3))\nar3=unname(round(fit$coef[\"ar3\"],3))\nma1=unname(round(fit$coef[\"ma1\"],3))\n```\n:::\n\n\n-   The fitted model is:\n\n$$y_t=1.047y_{t-1}-0.094y_{t-2}-0.003y_{t-3} +-1\\varepsilon_{t-1}+0.008$$\n\n-   The standard errors are 0.13, 0.06, 0.05, 0.12 and 0.002,\n    respectively.\n\n-   This suggest that only the AR1 and the constant (mean) are more than\n    2 SEs away from zero and thus statistically significant.\n\n-   The significance of $\\phi_0$ of this entertained model implies that\n    the expected mean return of the series is positive.\n\n-   In fact\n    $\\hat{\\mu}=0.006/(1-(1.047-0.094-0.003)) =0.12$\n    which is small but has long term implications.\n\n-   Using the multi-period return definition from the financial data\n    lecture an annualised log return is simple $\\sum_1^{12} y_t$\n    $\\approx 1.44$ per annum.\n\n## Frequentist prediction intervals {.small}\n\n$$\\hat{y}_{T+h|T} \\pm 1.96\\sqrt{v_{T+h|T}}$$ where $v_{T+h|T}$ is\nestimated forecast variance.\n\n-   $v_{T+1|T}=\\hat{\\sigma}^2$ for all ARIMA models regardless of\n    parameters and orders.\n-   Multi-step prediction intervals for ARIMA(0,0,$q$):\n\n$$\\displaystyle y_t = \\varepsilon_t + \\sum_{i=1}^q \\theta_i \\varepsilon_{t-i}$$\n\n$$\\displaystyle v_{T|T+h} = \\hat{\\sigma}^2 \\left[ 1 + \\sum_{i=1}^{h-1} \\theta_i^2\\right], \\qquad\\text{for~} h=2,3,\\dots.$$\n\n-   AR(1): Rewrite as MA($\\infty$) and use above result.\n-   Other models beyond scope of this subject.\n\n## Prediction intervals {.small}\n\n-   Prediction intervals **increase in size with forecast horizon**.\n-   Prediction intervals can be difficult to calculate by hand\n-   Calculations assume residuals are **uncorrelated** and **normally\n    distributed**.\n-   Prediction intervals tend to be too narrow.\n    -   the uncertainty in the parameter estimates has not been\n        accounted for.\n    -   the ARIMA model assumes historical patterns will not change\n        during the forecast period.\n    -   the ARIMA model assumes uncorrelated future $errors$\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}